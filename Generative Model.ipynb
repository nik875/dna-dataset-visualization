{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43de154",
   "metadata": {},
   "source": [
    "# Generative Model\n",
    "\n",
    "## 5:34 PM 6.9.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139ab36e-f911-4879-828c-4c3ecb7e612e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac0d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in plt.get_fignums():\n",
    "    plt.close(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c65415",
   "metadata": {},
   "source": [
    "## Load SILVA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c8497c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da2f850ad514a8ea5b27c15fa2ef474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "s = np.array([record for record in tqdm(SeqIO.parse('silva.fasta', \"fasta\"))], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a801659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b0565364e24224b3f8bc9f404fa44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from tqdm.notebook import tqdm\n",
    "def fn(i):\n",
    "    return np.array(list(str(i.seq)[:300]))\n",
    "with mp.Pool() as p:\n",
    "    string_seqs = np.array(list(tqdm(p.imap(fn, s, chunksize=100), total=s.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea5ec8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f24354ffe434070bf544ecd1961d9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASES = ['A', 'U', 'G', 'C']\n",
    "def fn(i):\n",
    "    enc_seq = np.empty((300, 5), dtype=np.intc)\n",
    "    for bp in range(string_seqs.shape[1]):\n",
    "        idx = BASES.index(i[bp]) if i[bp] in BASES else 4\n",
    "        enc_seq[bp] = [1 if j == idx else 0 for j in range(5)]\n",
    "    return enc_seq\n",
    "with mp.Pool() as p:\n",
    "    seqs = np.asarray(list(tqdm(p.imap(fn, string_seqs, chunksize=100), total=string_seqs.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b7dc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = np.array([i.description.split(' ')[1] for i in s])\n",
    "num_items = np.vectorize(lambda i: len(i.split(';')))(desc)\n",
    "parsable = num_items == 7\n",
    "raw_tax = desc[parsable]\n",
    "tax = []\n",
    "for i in raw_tax:\n",
    "    tax.append(i.split(';'))\n",
    "tax = np.array(tax)\n",
    "seqs = seqs[parsable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7276f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(seqs, test_size=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798cbab2",
   "metadata": {},
   "source": [
    "## VAE Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa62188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e801599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fed7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "55963e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_26 (InputLayer)          [(None, 300, 5)]     0           []                               \n",
      "                                                                                                  \n",
      " dense_107 (Dense)              (None, 300, 5)       30          ['input_26[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_42 (Reshape)           (None, 100, 15)      0           ['dense_107[0][0]']              \n",
      "                                                                                                  \n",
      " transformer_block_32 (Transfor  (None, 100, 15)     6970        ['reshape_42[0][0]']             \n",
      " merBlock)                                                                                        \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 100, 15)     60          ['transformer_block_32[0][0]']   \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 98, 20)       920         ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling1d_25 (MaxPooling1D  (None, 49, 20)      0           ['conv1d_25[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_37 (Flatten)           (None, 980)          0           ['max_pooling1d_25[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 980)         3920        ['flatten_37[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 50)           49050       ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 50)           49050       ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " sampling_11 (Sampling)         (None, 50)           0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,000\n",
      "Trainable params: 108,010\n",
      "Non-trainable params: 1,990\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 50\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    inputs = layers.Input((300, 5))\n",
    "    den = layers.Dense(5)(inputs)\n",
    "    res = layers.Reshape((100, 5 * 3))(den)\n",
    "    \n",
    "#     conv = layers.Conv1D(20, 3)(res)\n",
    "#     maxpool = layers.MaxPooling1D()(conv)\n",
    "#     res2 = layers.Flatten()(maxpool)\n",
    "#     norm = layers.BatchNormalization()(res2)\n",
    "#     den = layers.Dense(100 * 50 * 3)(norm)\n",
    "#     res = layers.Reshape((100, 50 * 3))(den)\n",
    "\n",
    "    trans = TransformerBlock(5 * 3, 4, 100)(res)\n",
    "    norm = layers.BatchNormalization()(trans)\n",
    "\n",
    "#     trans = TransformerBlock(50 * 3, 4, 100)(norm)\n",
    "#     norm = layers.BatchNormalization()(trans)\n",
    "\n",
    "    conv = layers.Conv1D(20, 3)(norm)\n",
    "    maxpool = layers.MaxPooling1D()(conv)\n",
    "    res2 = layers.Flatten()(maxpool)\n",
    "    norm = layers.BatchNormalization()(res2)\n",
    "\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(norm)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(norm)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "de2c4194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_27 (InputLayer)       [(None, 50)]              0         \n",
      "                                                                 \n",
      " flatten_38 (Flatten)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 1500)              76500     \n",
      "                                                                 \n",
      " reshape_43 (Reshape)        (None, 100, 15)           0         \n",
      "                                                                 \n",
      " transformer_block_33 (Trans  (None, 100, 15)          6970      \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 100, 15)          60        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_26 (Conv1D)          (None, 98, 20)            920       \n",
      "                                                                 \n",
      " max_pooling1d_26 (MaxPoolin  (None, 49, 20)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_39 (Flatten)        (None, 980)               0         \n",
      "                                                                 \n",
      " batch_normalization_60 (Bat  (None, 980)              3920      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 1500)              1471500   \n",
      "                                                                 \n",
      " reshape_44 (Reshape)        (None, 5, 300)            0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 5, 300)            90300     \n",
      "                                                                 \n",
      " reshape_45 (Reshape)        (None, 300, 5)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,650,170\n",
      "Trainable params: 1,648,180\n",
      "Non-trainable params: 1,990\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    flat = layers.Flatten()(latent_inputs)\n",
    "    den = layers.Dense(100 * 5 * 3)(flat)\n",
    "    res = layers.Reshape((100, 5 * 3))(den)\n",
    "    \n",
    "#     conv = layers.Conv1D(20, 3)(res)\n",
    "#     maxpool = layers.MaxPooling1D()(conv)\n",
    "#     res2 = layers.Flatten()(maxpool)\n",
    "#     norm = layers.BatchNormalization()(res2)\n",
    "#     den = layers.Dense(100 * 50 * 3)(norm)\n",
    "#     res = layers.Reshape((100, 50 * 3))(den)\n",
    "\n",
    "    trans = TransformerBlock(5 * 3, 4, 100)(res)\n",
    "    norm = layers.BatchNormalization()(trans)\n",
    "\n",
    "#     trans = TransformerBlock(50 * 3, 4, 100)(norm)\n",
    "#     norm = layers.BatchNormalization()(trans)\n",
    "\n",
    "    conv = layers.Conv1D(20, 3)(norm)\n",
    "    maxpool = layers.MaxPooling1D()(conv)\n",
    "    res2 = layers.Flatten()(maxpool)\n",
    "    norm = layers.BatchNormalization()(res2)\n",
    "    \n",
    "    den = layers.Dense(300 * 5)(norm)\n",
    "    res = layers.Reshape((5, 300))(den)\n",
    "    mid = layers.Dense(300, activation='sigmoid')(res)\n",
    "    out = layers.Reshape((300, 5))(mid)\n",
    "    \n",
    "    decoder = keras.Model(latent_inputs, out, name=\"decoder\")\n",
    "    decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f38affe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=1\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss ** 2\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        reconstruction = self.decoder(z)\n",
    "        \n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                keras.losses.binary_crossentropy(data, reconstruction), axis=1\n",
    "            )\n",
    "        )\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        total_loss = reconstruction_loss + kl_loss ** 2\n",
    "        \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def __call__(self, data, training=False):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "e6c379ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800e523e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "aee79030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "  1/163 [..............................] - ETA: 9s - loss: 61.5986 - reconstruction_loss: 42.5179 - kl_loss: 4.3679"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 14:03:37.711717: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1125419\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:8031\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - ETA: 0s - loss: 61.8071 - reconstruction_loss: 43.1830 - kl_loss: 4.3006"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 14:03:40.847045: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1126615\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\023FlatMapDataset:8061\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 3s 20ms/step - loss: 61.8063 - reconstruction_loss: 43.1830 - kl_loss: 4.3006 - val_loss: 61.8876 - val_reconstruction_loss: 43.2034 - val_kl_loss: 4.3215\n",
      "Epoch 2/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 61.7177 - reconstruction_loss: 43.0467 - kl_loss: 4.3106 - val_loss: 62.2984 - val_reconstruction_loss: 44.0461 - val_kl_loss: 4.2712\n",
      "Epoch 3/60\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 61.6810 - reconstruction_loss: 43.0751 - kl_loss: 4.3103 - val_loss: 62.1550 - val_reconstruction_loss: 43.3342 - val_kl_loss: 4.3372\n",
      "Epoch 4/60\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 61.6872 - reconstruction_loss: 42.9718 - kl_loss: 4.3121 - val_loss: 61.9949 - val_reconstruction_loss: 43.3922 - val_kl_loss: 4.3121\n",
      "Epoch 5/60\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 61.3971 - reconstruction_loss: 42.8413 - kl_loss: 4.3140 - val_loss: 61.9372 - val_reconstruction_loss: 43.0665 - val_kl_loss: 4.3430\n",
      "Epoch 6/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 61.6312 - reconstruction_loss: 42.8587 - kl_loss: 4.3184 - val_loss: 61.7785 - val_reconstruction_loss: 42.6400 - val_kl_loss: 4.3737\n",
      "Epoch 7/60\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 61.5269 - reconstruction_loss: 42.8108 - kl_loss: 4.3225 - val_loss: 62.1898 - val_reconstruction_loss: 43.5957 - val_kl_loss: 4.3110\n",
      "Epoch 8/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 61.4788 - reconstruction_loss: 42.7955 - kl_loss: 4.3250 - val_loss: 61.7486 - val_reconstruction_loss: 43.8038 - val_kl_loss: 4.2351\n",
      "Epoch 9/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 61.3111 - reconstruction_loss: 42.6524 - kl_loss: 4.3246 - val_loss: 61.9631 - val_reconstruction_loss: 43.4012 - val_kl_loss: 4.3074\n",
      "Epoch 10/60\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 61.5398 - reconstruction_loss: 42.6681 - kl_loss: 4.3303 - val_loss: 62.0177 - val_reconstruction_loss: 43.4464 - val_kl_loss: 4.3084\n",
      "Epoch 11/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 61.4182 - reconstruction_loss: 42.5724 - kl_loss: 4.3309 - val_loss: 61.5631 - val_reconstruction_loss: 42.8487 - val_kl_loss: 4.3250\n",
      "Epoch 12/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.9523 - reconstruction_loss: 42.4457 - kl_loss: 4.3275 - val_loss: 61.4085 - val_reconstruction_loss: 43.4650 - val_kl_loss: 4.2349\n",
      "Epoch 13/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 61.0925 - reconstruction_loss: 42.3956 - kl_loss: 4.3271 - val_loss: 61.5429 - val_reconstruction_loss: 42.6382 - val_kl_loss: 4.3469\n",
      "Epoch 14/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 61.0944 - reconstruction_loss: 42.4101 - kl_loss: 4.3330 - val_loss: 61.9459 - val_reconstruction_loss: 42.9296 - val_kl_loss: 4.3597\n",
      "Epoch 15/60\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 61.1713 - reconstruction_loss: 42.3053 - kl_loss: 4.3383 - val_loss: 61.1851 - val_reconstruction_loss: 42.8586 - val_kl_loss: 4.2798\n",
      "Epoch 16/60\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 60.9768 - reconstruction_loss: 42.2442 - kl_loss: 4.3365 - val_loss: 61.3433 - val_reconstruction_loss: 43.2164 - val_kl_loss: 4.2565\n",
      "Epoch 17/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 61.2099 - reconstruction_loss: 42.3383 - kl_loss: 4.3381 - val_loss: 61.6515 - val_reconstruction_loss: 42.7457 - val_kl_loss: 4.3471\n",
      "Epoch 18/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 61.0809 - reconstruction_loss: 42.1646 - kl_loss: 4.3369 - val_loss: 61.4138 - val_reconstruction_loss: 42.8953 - val_kl_loss: 4.3022\n",
      "Epoch 19/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 61.1508 - reconstruction_loss: 42.1118 - kl_loss: 4.3427 - val_loss: 61.4267 - val_reconstruction_loss: 42.6650 - val_kl_loss: 4.3303\n",
      "Epoch 20/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.9900 - reconstruction_loss: 42.0535 - kl_loss: 4.3470 - val_loss: 61.5562 - val_reconstruction_loss: 42.8818 - val_kl_loss: 4.3203\n",
      "Epoch 21/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 61.4118 - reconstruction_loss: 42.1280 - kl_loss: 4.3579 - val_loss: 61.1892 - val_reconstruction_loss: 42.9351 - val_kl_loss: 4.2714\n",
      "Epoch 22/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.9963 - reconstruction_loss: 41.9872 - kl_loss: 4.3469 - val_loss: 61.2446 - val_reconstruction_loss: 41.9833 - val_kl_loss: 4.3877\n",
      "Epoch 23/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.7566 - reconstruction_loss: 41.9889 - kl_loss: 4.3592 - val_loss: 61.8880 - val_reconstruction_loss: 43.3930 - val_kl_loss: 4.2995\n",
      "Epoch 24/60\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 60.7144 - reconstruction_loss: 41.8846 - kl_loss: 4.3536 - val_loss: 61.3645 - val_reconstruction_loss: 43.2795 - val_kl_loss: 4.2515\n",
      "Epoch 25/60\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 60.3079 - reconstruction_loss: 41.8187 - kl_loss: 4.3467 - val_loss: 60.8782 - val_reconstruction_loss: 42.2916 - val_kl_loss: 4.3101\n",
      "Epoch 26/60\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 60.7050 - reconstruction_loss: 41.8070 - kl_loss: 4.3575 - val_loss: 61.0764 - val_reconstruction_loss: 41.9727 - val_kl_loss: 4.3696\n",
      "Epoch 27/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.7504 - reconstruction_loss: 41.7154 - kl_loss: 4.3673 - val_loss: 61.0156 - val_reconstruction_loss: 42.3188 - val_kl_loss: 4.3229\n",
      "Epoch 28/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.8515 - reconstruction_loss: 41.7584 - kl_loss: 4.3558 - val_loss: 61.0807 - val_reconstruction_loss: 41.8801 - val_kl_loss: 4.3807\n",
      "Epoch 29/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.7986 - reconstruction_loss: 41.7381 - kl_loss: 4.3629 - val_loss: 61.5253 - val_reconstruction_loss: 42.5141 - val_kl_loss: 4.3591\n",
      "Epoch 30/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.6781 - reconstruction_loss: 41.6318 - kl_loss: 4.3635 - val_loss: 61.0351 - val_reconstruction_loss: 41.4171 - val_kl_loss: 4.4282\n",
      "Epoch 31/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.7223 - reconstruction_loss: 41.7214 - kl_loss: 4.3561 - val_loss: 61.3987 - val_reconstruction_loss: 42.5330 - val_kl_loss: 4.3423\n",
      "Epoch 32/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.7178 - reconstruction_loss: 41.6114 - kl_loss: 4.3577 - val_loss: 61.0984 - val_reconstruction_loss: 42.4148 - val_kl_loss: 4.3213\n",
      "Epoch 33/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.8010 - reconstruction_loss: 41.5265 - kl_loss: 4.3622 - val_loss: 60.8203 - val_reconstruction_loss: 42.2749 - val_kl_loss: 4.3053\n",
      "Epoch 34/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.6377 - reconstruction_loss: 41.5634 - kl_loss: 4.3716 - val_loss: 61.1823 - val_reconstruction_loss: 41.2694 - val_kl_loss: 4.4614\n",
      "Epoch 35/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.4522 - reconstruction_loss: 41.4536 - kl_loss: 4.3727 - val_loss: 60.6142 - val_reconstruction_loss: 42.0869 - val_kl_loss: 4.3032\n",
      "Epoch 36/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.6997 - reconstruction_loss: 41.4215 - kl_loss: 4.3725 - val_loss: 60.7466 - val_reconstruction_loss: 42.2091 - val_kl_loss: 4.3043\n",
      "Epoch 37/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.4700 - reconstruction_loss: 41.4069 - kl_loss: 4.3677 - val_loss: 61.3444 - val_reconstruction_loss: 42.4762 - val_kl_loss: 4.3426\n",
      "Epoch 38/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.7451 - reconstruction_loss: 41.4546 - kl_loss: 4.3775 - val_loss: 61.0353 - val_reconstruction_loss: 41.9882 - val_kl_loss: 4.3631\n",
      "Epoch 39/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.5300 - reconstruction_loss: 41.3573 - kl_loss: 4.3765 - val_loss: 61.3092 - val_reconstruction_loss: 41.8772 - val_kl_loss: 4.4071\n",
      "Epoch 40/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 3s 19ms/step - loss: 60.5028 - reconstruction_loss: 41.3035 - kl_loss: 4.3756 - val_loss: 60.9778 - val_reconstruction_loss: 41.7094 - val_kl_loss: 4.3885\n",
      "Epoch 41/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.5754 - reconstruction_loss: 41.3224 - kl_loss: 4.3742 - val_loss: 60.9734 - val_reconstruction_loss: 41.4979 - val_kl_loss: 4.4121\n",
      "Epoch 42/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.4970 - reconstruction_loss: 41.3450 - kl_loss: 4.3760 - val_loss: 61.4459 - val_reconstruction_loss: 42.1590 - val_kl_loss: 4.3905\n",
      "Epoch 43/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.7860 - reconstruction_loss: 41.2364 - kl_loss: 4.3886 - val_loss: 61.0216 - val_reconstruction_loss: 42.2961 - val_kl_loss: 4.3261\n",
      "Epoch 44/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.4447 - reconstruction_loss: 41.1933 - kl_loss: 4.3825 - val_loss: 60.8772 - val_reconstruction_loss: 41.6178 - val_kl_loss: 4.3874\n",
      "Epoch 45/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.3391 - reconstruction_loss: 41.1213 - kl_loss: 4.3783 - val_loss: 60.9122 - val_reconstruction_loss: 41.3323 - val_kl_loss: 4.4237\n",
      "Epoch 46/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.2803 - reconstruction_loss: 41.0226 - kl_loss: 4.3819 - val_loss: 61.3025 - val_reconstruction_loss: 41.6553 - val_kl_loss: 4.4313\n",
      "Epoch 47/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.3643 - reconstruction_loss: 41.1260 - kl_loss: 4.3901 - val_loss: 60.6787 - val_reconstruction_loss: 42.2559 - val_kl_loss: 4.2910\n",
      "Epoch 48/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.3683 - reconstruction_loss: 41.0358 - kl_loss: 4.3872 - val_loss: 61.3436 - val_reconstruction_loss: 42.2140 - val_kl_loss: 4.3726\n",
      "Epoch 49/60\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 60.3226 - reconstruction_loss: 41.0125 - kl_loss: 4.3925 - val_loss: 61.0076 - val_reconstruction_loss: 41.8638 - val_kl_loss: 4.3742\n",
      "Epoch 50/60\n",
      "  1/163 [..............................] - ETA: 2s - loss: 60.6041 - reconstruction_loss: 41.1254 - kl_loss: 4.4134"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1900700/3101140761.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae.fit(train, validation_data=(val,), epochs=60, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "7b0b896a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, multi_head_attention_32_layer_call_fn, multi_head_attention_32_layer_call_and_return_conditional_losses, layer_normalization_64_layer_call_fn, layer_normalization_64_layer_call_and_return_conditional_losses while saving (showing 5 of 23). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, multi_head_attention_33_layer_call_fn, multi_head_attention_33_layer_call_and_return_conditional_losses, layer_normalization_66_layer_call_fn, layer_normalization_66_layer_call_and_return_conditional_losses while saving (showing 5 of 23). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "# vae.save('Models/vae/full_model')\n",
    "encoder.save('Models/vae/encoder')\n",
    "decoder.save('Models/vae/decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec0dcf",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "39dcb12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.042191874>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean((val[:10000] - vae(val[:10000])) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "cb5da194",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.hist(encoder(val)[2].numpy().mean(axis=-1))\n",
    "plt.savefig('out.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1496d14",
   "metadata": {},
   "source": [
    "## Threshold Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e105081b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93142522ed6420191e700360c126627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.16497333333333333"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "a, b = np.unique(tax[:, 6], return_counts=True)\n",
    "species = a[b > 1]\n",
    "rng = np.random.default_rng()\n",
    "pairs = []\n",
    "for i in tqdm(range(500)):\n",
    "    spec = rng.choice(species, 1)\n",
    "    choose_from = seqs[tax[:, 6] == spec]\n",
    "    pair = rng.choice(choose_from, 2, replace=False)\n",
    "    pairs.append(pair)\n",
    "pairs = np.array(pairs)\n",
    "((pairs[:, 0] - pairs[:, 1]) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1a74fdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.121285945, 0.16438128)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = encoder(val)[2]\n",
    "b = a + tf.keras.backend.random_normal(shape=a.shape) * .95\n",
    "pair_diff = tf.reduce_mean((decoder(a) - decoder(b)) ** 2).numpy()\n",
    "init_diff = tf.reduce_mean((decoder(b) - val) ** 2).numpy()\n",
    "pair_diff, init_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8b570",
   "metadata": {},
   "source": [
    "Optimal threshold: .95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a6bff",
   "metadata": {},
   "source": [
    "## Species-Level Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5513fa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 14:21:02.478606: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-12 14:21:03.261867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:21:00.0, compute capability: 8.0\n",
      "2022-06-12 14:21:03.263300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38420 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "encoder = tf.keras.models.load_model('Models/vae/encoder')\n",
    "decoder = tf.keras.models.load_model('Models/vae/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "682c5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_similar(arr):\n",
    "    a = encoder(arr)[2]\n",
    "    b = a + tf.keras.backend.random_normal(shape=a.shape) * .95\n",
    "    b_dec = decoder(b)\n",
    "    return tf.math.round(b_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970fdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
