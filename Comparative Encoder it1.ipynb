{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43de154",
   "metadata": {},
   "source": [
    "# Comparative Encoder\n",
    "\n",
    "## 5:34 PM 6.9.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "139ab36e-f911-4879-828c-4c3ecb7e612e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e282982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in plt.get_fignums():\n",
    "    plt.close(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c65415",
   "metadata": {},
   "source": [
    "## Load SILVA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a7c8497c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e574f325c404f9baae103a94e72ab30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "s = np.array([record for record in tqdm(SeqIO.parse('silva.fasta', \"fasta\"))], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a801659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9c30b491e346d5bace0dc6a8d06b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from tqdm.notebook import tqdm\n",
    "def fn(i):\n",
    "    return np.array(list(str(i.seq)[:300]))\n",
    "with mp.Pool() as p:\n",
    "    string_seqs = np.array(list(tqdm(p.imap(fn, s, chunksize=100), total=s.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ea5ec8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efe1f80ca5f4944a4347d8a05c003b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/227331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASES = ['A', 'U', 'G', 'C']\n",
    "def fn(i):\n",
    "    enc_seq = np.empty((300, 5), dtype=np.intc)\n",
    "    for bp in range(string_seqs.shape[1]):\n",
    "        idx = BASES.index(i[bp]) if i[bp] in BASES else 4\n",
    "        enc_seq[bp] = [1 if j == idx else 0 for j in range(5)]\n",
    "    return enc_seq\n",
    "with mp.Pool() as p:\n",
    "    seqs = np.asarray(list(tqdm(p.imap(fn, string_seqs, chunksize=100), total=string_seqs.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1b7dc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = np.array([i.description.split(' ')[1] for i in s])\n",
    "num_items = np.vectorize(lambda i: len(i.split(';')))(desc)\n",
    "parsable = num_items == 7\n",
    "raw_tax = desc[parsable]\n",
    "tax = []\n",
    "for i in raw_tax:\n",
    "    tax.append(i.split(';'))\n",
    "tax = np.array(tax)\n",
    "seqs = seqs[parsable]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc1471",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9f341f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0186f58dee5c4af78e7b868904c7b648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/180516 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "codes = BASES + ['N']\n",
    "def to_str(s):\n",
    "    return ''.join(codes[i] for i in s)\n",
    "\n",
    "def decode(sample):\n",
    "    str_seq_samp = []\n",
    "    for i in tqdm(sample):\n",
    "        str_seq_samp.append(to_str(i))\n",
    "    return np.asarray(str_seq_samp)\n",
    "str_seqs = decode(seqs.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a7a5a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seqs_train, seqs_val, str_seqs_train, str_seqs_val = train_test_split(seqs, str_seqs, test_size=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7508db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = np.array(np.meshgrid(np.arange(seqs_val.shape[0]), np.arange(seqs_val.shape[0]))).T.reshape(-1, 2)\n",
    "val_x1 = seqs_val[pairs[:, 0]]\n",
    "val_x2 = seqs_val[pairs[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "26e812ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93fd83f9eab4fccbc2e3ef7ef55816a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3261636 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Bio import pairwise2\n",
    "def dissimilarity(pair):\n",
    "    return (1 / (pairwise2.align.localxx(str_seqs_val[pair[0]], str_seqs_val[pair[1]], score_only=True) / 300)) - 1\n",
    "\n",
    "import multiprocessing\n",
    "with multiprocessing.Pool() as p:\n",
    "    val_labels = np.array(list(tqdm(p.imap(dissimilarity, pairs, chunksize=1000), total=pairs.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c4ea7a",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ec66c9d9-90f9-4659-a238-77c0635277f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cd24ebfc-20bf-4616-8a56-6522d162f8f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, a, b):\n",
    "        return tf.reduce_sum(tf.square(a - b), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a062448a-0310-4fbc-8061-9c926b9da302",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 300, 5)]          0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 300, 50)           300       \n",
      "                                                                 \n",
      " reshape_10 (Reshape)        (None, 100, 150)          0         \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 98, 20)            9020      \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 49, 20)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 980)               0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 980)              3920      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 15000)             14715000  \n",
      "                                                                 \n",
      " reshape_11 (Reshape)        (None, 100, 150)          0         \n",
      "                                                                 \n",
      " transformer_block_6 (Transf  (None, 100, 150)         392800    \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 100, 150)         600       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " transformer_block_7 (Transf  (None, 100, 150)         392800    \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 100, 150)         600       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 98, 20)            9020      \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPooling  (None, 49, 20)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 980)               0         \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 980)              3920      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 2)                 1962      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,529,942\n",
      "Trainable params: 15,525,422\n",
      "Non-trainable params: 4,520\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    inputs = layers.Input((300, 5))\n",
    "    den = layers.Dense(50)(inputs)\n",
    "    res = layers.Reshape((100, 50 * 3))(den)\n",
    "    \n",
    "    conv = layers.Conv1D(20, 3)(res)\n",
    "    maxpool = layers.MaxPooling1D()(conv)\n",
    "    res2 = layers.Flatten()(maxpool)\n",
    "    norm = layers.BatchNormalization()(res2)\n",
    "    den = layers.Dense(100 * 50 * 3)(norm)\n",
    "    res = layers.Reshape((100, 50 * 3))(den)\n",
    "\n",
    "    trans = TransformerBlock(50 * 3, 4, 100)(res)\n",
    "    norm = layers.BatchNormalization()(trans)\n",
    "\n",
    "    trans = TransformerBlock(50 * 3, 4, 100)(norm)\n",
    "    norm = layers.BatchNormalization()(trans)\n",
    "\n",
    "    conv = layers.Conv1D(20, 3)(norm)\n",
    "    maxpool = layers.MaxPooling1D()(conv)\n",
    "    res2 = layers.Flatten()(maxpool)\n",
    "    norm = layers.BatchNormalization()(res2)\n",
    "\n",
    "    out = layers.Dense(2)(norm)\n",
    "\n",
    "    embeddings = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "embeddings.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bb0b4d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_a (InputLayer)           [(None, 300, 5)]     0           []                               \n",
      "                                                                                                  \n",
      " input_b (InputLayer)           [(None, 300, 5)]     0           []                               \n",
      "                                                                                                  \n",
      " model_9 (Functional)           (None, 2)            15529942    ['input_a[0][0]',                \n",
      "                                                                  'input_b[0][0]']                \n",
      "                                                                                                  \n",
      " distance_layer_4 (DistanceLaye  (None,)             0           ['model_9[0][0]',                \n",
      " r)                                                               'model_9[1][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,529,942\n",
      "Trainable params: 15,525,422\n",
      "Non-trainable params: 4,520\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "def correlation_coefficient_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return 1 - K.square(r)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return correlation_coefficient_loss(y_true, y_pred) + tf.keras.losses.MeanSquaredError(\n",
    "        tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    inputa = layers.Input((300, 5), name='input_a')\n",
    "    inputb = layers.Input((300, 5), name='input_b')\n",
    "    distances = DistanceLayer()(\n",
    "        embeddings(inputa),\n",
    "        embeddings(inputb),\n",
    "    )\n",
    "    siamese_network = tf.keras.Model(inputs=[inputa, inputb], outputs=distances)\n",
    "    siamese_network.compile(optimizer='adam',\n",
    "                    loss=combined_loss,\n",
    "                    metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b429cc6",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b56a43cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'combined_loss': combined_loss}\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    siamese_network = tf.keras.models.load_model('Models/comparative_encoder/full_model')\n",
    "    embeddings = tf.keras.models.load_model('Models/comparative_encoder/encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb35ed",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9cccdc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "def dissimilarity(pair):\n",
    "    return (1 / (pairwise2.align.localxx(pair[0], pair[1], score_only=True) / 300)) - 1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "rng = np.random.default_rng()\n",
    "def randomized_epoch(data, str_data):\n",
    "    x1, x2, x1_str, x2_str = train_test_split(data, str_data, test_size=.5, random_state=0)\n",
    "    \n",
    "    import multiprocessing\n",
    "    with multiprocessing.Pool() as p:\n",
    "        y = np.array(list(tqdm(p.imap(dissimilarity, zip(x1_str, x2_str), chunksize=1000), total=x1_str.shape[0])))\n",
    "    \n",
    "    train_data = tf.data.Dataset.from_tensor_slices(({'input_a': x1, 'input_b': x2}, y))\n",
    "    train_data = train_data.batch(1000)\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    train_data = train_data.with_options(options)\n",
    "    \n",
    "    siamese_network.fit(train_data, epochs=1)\n",
    "\n",
    "def validate():\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((\n",
    "        {'input_a': val_x1[:100000], 'input_b': val_x2[:100000]},))\n",
    "    val_data = val_data.batch(1000)\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    val_data = val_data.with_options(options)\n",
    "    pred = siamese_network.predict(val_data)\n",
    "    return np.corrcoef(pred, val_labels[:100000])[0, 1] ** 2, ((pred - val_labels[:100000]) ** 2).mean()\n",
    "\n",
    "def train(epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f'Epoch {i + 1}:')\n",
    "        randomized_epoch(seqs_train, str_seqs_train)\n",
    "        val_r, val_mse = validate()\n",
    "        print(f'val_mse: {val_mse}; val_r2: {val_r}')\n",
    "        siamese_network.save('Models/comparative_encoder/full_model')\n",
    "        embeddings.save('Models/comparative_encoder/encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "430c0584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7b832e782248189cc8fcf42b20e39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 20s 91ms/step - loss: 1.8406 - mean_absolute_error: 0.4181\n",
      "100/100 [==============================] - 5s 32ms/step\n",
      "val_mse: 0.1130027553179244; val_r2: 0.003927908216945577\n",
      "Epoch 2:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61269c135b0495db970b76f641b20e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 8s 91ms/step - loss: 1.1164 - mean_absolute_error: 0.2534\n",
      "100/100 [==============================] - 3s 31ms/step\n",
      "val_mse: 0.11232928356276786; val_r2: 0.004199364455520755\n",
      "Epoch 3:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac06df1907d4299a808333b0116d7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 8s 90ms/step - loss: 1.0831 - mean_absolute_error: 0.2282\n",
      "100/100 [==============================] - 4s 31ms/step\n",
      "val_mse: 0.11195188866779204; val_r2: 0.007479852569580837\n",
      "Epoch 4:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a67669eecc4a3189474ae38300be4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 8s 90ms/step - loss: 1.0720 - mean_absolute_error: 0.2201\n",
      "100/100 [==============================] - 3s 31ms/step\n",
      "val_mse: 0.11200208549130619; val_r2: 0.0064317774553822185\n",
      "Epoch 5:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f004eb3626a347f5a40ace5e67c9d4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 8s 91ms/step - loss: 1.0677 - mean_absolute_error: 0.2167\n",
      "100/100 [==============================] - 3s 31ms/step\n",
      "val_mse: 0.11189072760260853; val_r2: 0.010331363003440477\n",
      "Epoch 6:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a639a3cacf4ca08c72044a5da35a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89355 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/90 [=========================>....] - ETA: 0s - loss: 1.0656 - mean_absolute_error: 0.2156"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1666560/3697265181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1666560/2206728051.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {i + 1}:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mrandomized_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_seqs_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mval_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'val_mse: {val_mse}; val_r2: {val_r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1666560/2206728051.py\u001b[0m in \u001b[0;36mrandomized_epoch\u001b[0;34m(data, str_data)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0msiamese_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a02219b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.trainable = False\n",
    "siamese_network.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96fbc59",
   "metadata": {},
   "source": [
    "## Evaluation on SILVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b112e0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 15:59:10.729210: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_3933676\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:10804\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5642/5642 [==============================] - 36s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "seq_reps = embeddings.predict(seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453873b9",
   "metadata": {},
   "source": [
    "### Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07f5a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.spatial.distance import euclidean\n",
    "codes = BASES + ['N']\n",
    "def to_str(s):\n",
    "    return ''.join(codes[i] for i in s)\n",
    "def evaluate():\n",
    "    a = random.randint(0, seq_reps.shape[0] - 1)\n",
    "    b = random.randint(0, seq_reps.shape[0] - 1)\n",
    "    pred = euclidean(seq_reps[a], seq_reps[b])\n",
    "    first = to_str(seqs[a].argmax(-1))\n",
    "    second = to_str(seqs[b].argmax(-1))\n",
    "    score = dissimilarity((first, second))\n",
    "    return [score, pred]\n",
    "results = np.asarray([evaluate() for i in range(10000)])\n",
    "f = plt.figure(figsize=(8, 6))\n",
    "plt.scatter(results[:, 0], results[:, 1], alpha=.2)\n",
    "plt.xlabel('Pairwise Similarity Score (Smith-Waterman)')\n",
    "plt.ylabel('Euclidean Distance Between Encodings')\n",
    "plt.title('Correlation Plot of Model Encodings')\n",
    "plt.savefig('Results/it1/silva/eval/comparative_enc_eval.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00a363ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.81438758],\n",
       "       [0.81438758, 1.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(results[:, 0], results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2c114d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07276923042824604"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((results[:, 0] - results[:, 1]) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90f047",
   "metadata": {},
   "source": [
    "### SILVA Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b374eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x, y = np.array(seq_reps).T\n",
    "f = plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, alpha=.1, marker='o')\n",
    "plt.title(\"Encoded Representations of SILVA Database\")\n",
    "plt.savefig('Results/it1/silva/silva_all.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de488cc2",
   "metadata": {},
   "source": [
    "#### Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84035c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "archaea = seq_reps[tax[:, 0] == 'Archaea']\n",
    "bacteria = seq_reps[tax[:, 0] == 'Bacteria']\n",
    "eukaryota = seq_reps[tax[:, 0] == 'Eukaryota']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02dd9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8, 6))\n",
    "plt.scatter(archaea[:, 0], archaea[:, 1], alpha=.4)\n",
    "plt.scatter(bacteria[:, 0], bacteria[:, 1], alpha=.015)\n",
    "plt.scatter(eukaryota[:, 0], eukaryota[:, 1], alpha=.4)\n",
    "leg = plt.legend(['Archaea', \n",
    "                  'Bacteria',\n",
    "                  'Eukaryota'],\n",
    "                markerscale=1,\n",
    "                borderpad=1)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "plt.title('Encoded Representations of the Domains of Life')\n",
    "plt.savefig('Results/it1/silva/silva_domains.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed8ff9d",
   "metadata": {},
   "source": [
    "#### Phylum (Bacteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50574968",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.unique(tax[tax[:, 0] == 'Bacteria'][:, 1], return_counts=True)\n",
    "genuses = a[b > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5d12d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable = np.isin(tax[:, 1], genuses)\n",
    "to_plot = np.zeros((np.nonzero(plottable)[0].shape[0], genuses.shape[0]))\n",
    "for i in range(len(genuses)):\n",
    "    to_plot[tax[plottable][:, 1] == genuses[i], i] = 1\n",
    "plottable_seqs = seq_reps[plottable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "589b3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in to_plot.T:\n",
    "    pop = plottable_seqs[i.astype(bool)]\n",
    "    samp = rng.integers(0, len(pop), 1000)\n",
    "    x, y = zip(*pop[samp])\n",
    "    ax.scatter(x, y, alpha=.3, marker='o')\n",
    "ax.set_title(\"Encoded Representations of Phyla of Bacteria\")\n",
    "leg = plt.legend(genuses,\n",
    "                markerscale=1,\n",
    "                borderpad=1)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "plt.savefig('Results/it1/silva/silva_phylum.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97293f09",
   "metadata": {},
   "source": [
    "#### Class (Proteobacteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4a3bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.unique(tax[tax[:, 1] == 'Proteobacteria'][:, 2], return_counts=True)\n",
    "genuses = a[b > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e68b8a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable = np.isin(tax[:, 2], genuses)\n",
    "to_plot = np.zeros((np.nonzero(plottable)[0].shape[0], genuses.shape[0]))\n",
    "for i in range(len(genuses)):\n",
    "    to_plot[tax[plottable][:, 2] == genuses[i], i] = 1\n",
    "plottable_seqs = seq_reps[plottable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1cf5e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in to_plot.T:\n",
    "    pop = plottable_seqs[i.astype(bool)]\n",
    "    samp = rng.integers(0, len(pop), 1000)\n",
    "    x, y = zip(*pop[samp])\n",
    "    ax.scatter(x, y, alpha=.3, marker='o')\n",
    "ax.set_title(\"Encoded Representations of Classes of Proteobacteria\")\n",
    "leg = plt.legend(genuses,\n",
    "                markerscale=1,\n",
    "                borderpad=1)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "plt.savefig('Results/it1/silva/silva_class.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626330a1",
   "metadata": {},
   "source": [
    "#### Order (Alphaproteobacteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2a3c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.unique(tax[tax[:, 2] == 'Alphaproteobacteria'][:, 3], return_counts=True)\n",
    "genuses = a[b > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa24c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable = np.isin(tax[:, 3], genuses)\n",
    "to_plot = np.zeros((np.nonzero(plottable)[0].shape[0], genuses.shape[0]))\n",
    "for i in range(len(genuses)):\n",
    "    to_plot[tax[plottable][:, 3] == genuses[i], i] = 1\n",
    "plottable_seqs = seq_reps[plottable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fff25814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in to_plot.T:\n",
    "    pop = plottable_seqs[i.astype(bool)]\n",
    "    samp = rng.integers(0, len(pop), 1000)\n",
    "    x, y = zip(*pop[samp])\n",
    "    ax.scatter(x, y, alpha=.3, marker='o')\n",
    "ax.set_title(\"Encoded Representations of Orders of Alphaproteobacteria\")\n",
    "leg = plt.legend(genuses,\n",
    "                markerscale=1,\n",
    "                borderpad=1)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "plt.savefig('Results/it1/silva/silva_order.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7dd48",
   "metadata": {},
   "source": [
    "#### Genus (Rhizobiales Rhizobiaceae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea822c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.unique(tax[tax[:, 4] == 'Rhizobiaceae'][:, 5], return_counts=True)\n",
    "genuses = a[b > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a985ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable = np.isin(tax[:, 5], genuses)\n",
    "to_plot = np.zeros((np.nonzero(plottable)[0].shape[0], genuses.shape[0]))\n",
    "for i in range(len(genuses)):\n",
    "    to_plot[tax[plottable][:, 5] == genuses[i], i] = 1\n",
    "plottable_seqs = seq_reps[plottable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf9a3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in to_plot.T:\n",
    "    pop = plottable_seqs[i.astype(bool)]\n",
    "    samp = rng.integers(0, len(pop), 1000)\n",
    "    x, y = zip(*pop[samp])\n",
    "    ax.scatter(x, y, alpha=.5, marker='o')\n",
    "ax.set_title(\"Encoded Representations of Orders of Alphaproteobacteria\")\n",
    "leg = plt.legend(genuses,\n",
    "                markerscale=1,\n",
    "                borderpad=1)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "plt.savefig('Results/it1/silva/silva_genus.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c26736",
   "metadata": {},
   "source": [
    "# ANC Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec473586",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef38d8c",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a381eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"S002\": \"black\",\n",
    "    \"S001\": \"grey\",\n",
    "    \"S003\": \"unpigmented\"\n",
    "}\n",
    "def read_ion_reporter(path):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from Bio import SeqIO\n",
    "    from tqdm.notebook import tqdm\n",
    "    import re\n",
    "\n",
    "    paths = np.array([])  # Get paths to all fasta files\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        paths = np.append(paths, [os.path.join(root, i) for i in files if i.endswith(\".fasta\")])\n",
    "\n",
    "    def read(i):  # Function to parse a fasta file and get all records\n",
    "        s = np.array([record for index, record in enumerate(SeqIO.parse(i, \"fasta\"))] + [0], dtype=object)\n",
    "        s = s[:-1]  # We append 0 and remove it so that single element arrays get parsed as such\n",
    "        l = np.full_like(s, label_map[i.split(\"/\")[-1][:4]])\n",
    "        return s, l\n",
    "\n",
    "    seqs, labels, desc = np.array([]), np.array([]), np.array([])\n",
    "    for i in paths:  # Parse all sequences, generate labels\n",
    "        s, l = read(i)\n",
    "        seqs = np.append(seqs, s)\n",
    "        labels = np.append(labels, l)\n",
    "    # flattened_seqs = np.concatenate(seqs)  # Flatten because we don't care about which sample the data came from\n",
    "    string_seqs = np.vectorize(lambda i: str(i.seq))(seqs)  # Convert to strings\n",
    "    descriptions = np.vectorize(lambda i: i.description)(seqs)\n",
    "\n",
    "    return string_seqs, labels, descriptions\n",
    "\n",
    "arlington, arlington_labels, arlington_desc = read_ion_reporter(\"Data/Arlington Processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d2f83",
   "metadata": {},
   "source": [
    "### Header Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4020afb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_size(s: str):\n",
    "    parts = s.split('|')\n",
    "    idx = -1\n",
    "    for i in range(len(parts)):\n",
    "        if re.search(r'\\.', parts[i]):\n",
    "            idx = i\n",
    "            break\n",
    "    if idx == -1:\n",
    "        return 0\n",
    "    return len(parts[i + 1:])\n",
    "\n",
    "import numpy as np\n",
    "arlington_known = np.vectorize(get_size)(arlington_desc) != 0\n",
    "arlington = arlington[arlington_known]\n",
    "arlington_labels = arlington_labels[arlington_known]\n",
    "arlington_desc = arlington_desc[arlington_known]\n",
    "\n",
    "def get_tax(s: str):\n",
    "    parts = s.split('|')\n",
    "    for i in range(len(parts)):\n",
    "        if re.search(r'\\.', parts[i]):\n",
    "            break\n",
    "    tax = [x.strip('[]') for x in parts[i + 1:-1]]\n",
    "    unc = [\"UNKNOWN\"] * 6\n",
    "    cond = ['/' not in i and 'sp.' not in i for i in tax]\n",
    "    return np.where(cond, tax, unc)\n",
    "\n",
    "arlington_tax = np.empty((arlington_desc.shape[0], 6), dtype=object)\n",
    "for i in range(arlington_desc.shape[0]):\n",
    "    arlington_tax[i, :] = get_tax(arlington_desc[i])\n",
    "\n",
    "def get_conf(s: str):\n",
    "    parts = s.split('|')\n",
    "    for i in range(len(parts)):\n",
    "        if re.search(r'\\.', parts[i]):\n",
    "            break\n",
    "    return float(parts[i])\n",
    "\n",
    "arlington_conf = np.vectorize(get_conf)(arlington_desc)\n",
    "\n",
    "def get_abund(s: str):\n",
    "    parts = s.split('|')\n",
    "    for i in range(len(parts)):\n",
    "        if re.search(r'\\.', parts[i]):\n",
    "            break\n",
    "    return int(parts[i - 1])\n",
    "\n",
    "arlington_abund = np.vectorize(get_abund)(arlington_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce8ad1a",
   "metadata": {},
   "source": [
    "### Sequence Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ae59031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6b2b5c8c3741efb242c7c331471e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_reads(x):\n",
    "    from tqdm.notebook import tqdm\n",
    "    from sklearn import preprocessing\n",
    "    import numpy as np\n",
    "\n",
    "    BASES = [\"A\", \"T\", \"G\", \"C\"]\n",
    "\n",
    "    LENGTH = 300\n",
    "\n",
    "    # mask = np.vectorize(len)(x) >= LENGTH\n",
    "    seqs = np.vectorize(lambda i: i[:LENGTH])(x)\n",
    "\n",
    "    print(\"Encoding sequences...\")\n",
    "    # Sequence encoding\n",
    "    def encode_seq(seq: str):  # Function to encode a sequence using one-hot encoding\n",
    "        idx = [BASES.index(char) for char in seq]\n",
    "        return [[(1 if j == i else 0) for j in range(5)] for i in idx]\n",
    "    # Time consuming step, vectorize if possible!!!\n",
    "    final_seqs = np.empty((seqs.shape[0], LENGTH, 5), dtype=np.int32)  # Init empty array\n",
    "    for i in tqdm(range(len(seqs))):  # Iterate over sequences\n",
    "        encoded = encode_seq(seqs[i][:LENGTH])  # Encode each sequence\n",
    "        final_seqs[i] = np.concatenate([np.zeros((LENGTH - len(encoded), 5)), encoded])\n",
    "\n",
    "    return final_seqs\n",
    "\n",
    "arlington_processed = preprocess_reads(arlington)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aed58a",
   "metadata": {},
   "source": [
    "### Variable Region Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cffffb",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f3d96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable_region(i):  # Function to get the variable region from the record's description\n",
    "    header_parts = i.split(\"|\")  # Header is | delimited\n",
    "    variable_region = \"UNKNOWN\"  # Default variable region\n",
    "    for val in header_parts:  # Iterate over the header parts\n",
    "        if result := re.search(r\"^V\\d+\", val):  # Match any pattern starting with a V and ending with digits\n",
    "            variable_region = val  # Set variable region to the text in the matched part of the header\n",
    "            break\n",
    "    return variable_region\n",
    "\n",
    "import numpy as np\n",
    "v_regions = np.vectorize(get_variable_region)(arlington_desc)\n",
    "\n",
    "known_mask = v_regions != 'UNKNOWN'\n",
    "known_seqs, known_labels = arlington_processed[known_mask], v_regions[known_mask]\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "bn = LabelBinarizer()\n",
    "v_regions_enc = bn.fit_transform(known_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07773bd8",
   "metadata": {},
   "source": [
    "#### NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5cfb713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 300, 5)]          0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 300, 300)          1800      \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 90000)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 6)                 540006    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 541,806\n",
      "Trainable params: 541,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "inputs = tf.keras.layers.Input((300, 5))\n",
    "hidden = tf.keras.layers.Dense(300, activation='relu')(inputs)\n",
    "flat = tf.keras.layers.Flatten()(hidden)\n",
    "outputs = tf.keras.layers.Dense(v_regions_enc.shape[1], activation='softmax')(flat)\n",
    "\n",
    "v_region_classifier = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "v_region_classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "v_region_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e14ac356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "69/69 [==============================] - 1s 6ms/step - loss: 0.1185 - accuracy: 0.9686 - val_loss: 0.0147 - val_accuracy: 0.9965\n",
      "Epoch 2/100\n",
      "69/69 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 0.9991 - val_loss: 0.0053 - val_accuracy: 0.9983\n",
      "Epoch 3/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.0030 - val_accuracy: 0.9994\n",
      "Epoch 4/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 6.1241e-04 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
      "Epoch 5/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 3.9396e-04 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 0.9994\n",
      "Epoch 6/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.8428e-04 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 0.9994\n",
      "Epoch 7/100\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 2.1138e-04 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14c2493562b0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(known_seqs, v_regions_enc, test_size=.2)\n",
    "import tensorflow as tf\n",
    "v_region_classifier.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "                       callbacks=[tf.keras.callbacks.EarlyStopping(patience=1, monitor='val_loss')],\n",
    "                       batch_size=100,\n",
    "                       epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347125b",
   "metadata": {},
   "source": [
    "#### Classify All Seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "af2f75d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 [==============================] - 0s 865us/step\n",
      "(8571,) (8545,)\n",
      "0.9984832574962081\n"
     ]
    }
   ],
   "source": [
    "unknown_labels = v_region_classifier.predict(arlington_processed[~known_mask], verbose=1)\n",
    "int_predictions = unknown_labels.argmax(axis=1)\n",
    "predictions = np.vectorize(lambda i: bn.classes_[i])(int_predictions)\n",
    "certainty = unknown_labels.max(axis=1)\n",
    "print(certainty.shape, np.nonzero(certainty > .99)[0].shape)  # We can classify almost everything with 99% confidence\n",
    "\n",
    "v_region_labels = np.full_like(v_regions, 'UNKNOWN')\n",
    "v_region_labels[known_mask] = known_labels\n",
    "a = v_region_labels[~known_mask]\n",
    "a[certainty > .99] = predictions[certainty > .99]\n",
    "v_region_labels[~known_mask] = a\n",
    "print(v_region_labels[v_region_labels != 'UNKNOWN'].shape[0] / v_region_labels.shape[0])\n",
    "# We successfully classified about 99% of our unknown data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef8cad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('default')\n",
    "labels, sizes = np.unique(v_region_labels, return_counts=True)\n",
    "# s = np.argsort(sizes)\n",
    "# labels, sizes = labels[s], sizes[s]\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "ax.pie(sizes, labels=labels, startangle=90, autopct='%1.1f%%', explode=(.7, 0, 0, 0, 0, 0, .5))\n",
    "ax.axis('equal')\n",
    "ax.set_title('Variable Region Breakdown After Classification\\n')\n",
    "plt.savefig('ANC_vregion_breakdown.png')\n",
    "# plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47cb4a6",
   "metadata": {},
   "source": [
    "### Prep for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a2247d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acce89edaee64515af993265aaabc729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "codes = BASES + ['N']\n",
    "def to_str(s):\n",
    "    return ''.join(codes[i] for i in s)\n",
    "\n",
    "def decode(sample):\n",
    "    str_seq_samp = []\n",
    "    for i in tqdm(sample):\n",
    "        str_seq_samp.append(to_str(i))\n",
    "    return np.asarray(str_seq_samp)\n",
    "anc_str_seqs = decode(arlington_processed.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "54053e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "anc_seqs_train, anc_seqs_val, anc_str_seqs_train, anc_str_seqs_val = \\\n",
    "    train_test_split(arlington_processed, anc_str_seqs, test_size=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84872f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = np.array(np.meshgrid(np.arange(anc_seqs_val.shape[0]), np.arange(anc_seqs_val.shape[0]))).T.reshape(-1, 2)\n",
    "anc_val_x1 = anc_seqs_val[pairs[:, 0]]\n",
    "anc_val_x2 = anc_seqs_val[pairs[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b1759e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47dbd16aa1934e428db6d82935ca6bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Bio import pairwise2\n",
    "def dissimilarity(pair):\n",
    "    return (1 / (pairwise2.align.localxx(\n",
    "        anc_str_seqs_val[pair[0]], anc_str_seqs_val[pair[1]], score_only=True) / 300)) - 1\n",
    "\n",
    "anc_val_labels = np.array([dissimilarity(i) for i in tqdm(pairs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ea541",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338b1c83",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e12507c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 300, 5)]          0         \n",
      "                                                                 \n",
      " reshape_5 (Reshape)         (None, 5, 300)            0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 5, 300)            90300     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 5, 300)            90300     \n",
      "                                                                 \n",
      " reshape_6 (Reshape)         (None, 300, 5)            0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 2)                 15529942  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,710,542\n",
      "Trainable params: 180,600\n",
      "Non-trainable params: 15,529,942\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "with mirrored_strategy.scope():\n",
    "    anc_input = layers.Input((300, 5))\n",
    "    res = layers.Reshape((5, 300))(anc_input)\n",
    "    mid = layers.Dense(300)(res)\n",
    "    mid = layers.Dense(300)(mid)\n",
    "    res = layers.Reshape((300, 5))(mid)\n",
    "    embed = embeddings(res)\n",
    "\n",
    "    fine_embeddings = tf.keras.Model(inputs=anc_input, outputs=embed)\n",
    "\n",
    "fine_embeddings.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f723ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_a (InputLayer)           [(None, 300, 5)]     0           []                               \n",
      "                                                                                                  \n",
      " input_b (InputLayer)           [(None, 300, 5)]     0           []                               \n",
      "                                                                                                  \n",
      " model_3 (Functional)           (None, 2)            15710542    ['input_a[0][0]',                \n",
      "                                                                  'input_b[0][0]']                \n",
      "                                                                                                  \n",
      " distance_layer_1 (DistanceLaye  (None,)             0           ['model_3[0][0]',                \n",
      " r)                                                               'model_3[1][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,710,542\n",
      "Trainable params: 180,600\n",
      "Non-trainable params: 15,529,942\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "def correlation_coefficient_loss(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = K.mean(y)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "\n",
    "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
    "    return 1 - K.square(r)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return correlation_coefficient_loss(y_true, y_pred) + tf.keras.losses.MeanSquaredError(\n",
    "        tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    inputa = layers.Input((300, 5), name='input_a')\n",
    "    inputb = layers.Input((300, 5), name='input_b')\n",
    "    distances = DistanceLayer()(\n",
    "        fine_embeddings(inputa),\n",
    "        fine_embeddings(inputb),\n",
    "    )\n",
    "    fine_siamese = tf.keras.Model(inputs=[inputa, inputb], outputs=distances)\n",
    "    fine_siamese.compile(optimizer='adam',\n",
    "                    loss=combined_loss,\n",
    "                    metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "fine_siamese.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3eaaf",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e56e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "def dissimilarity(pair):\n",
    "    return (1 / (pairwise2.align.localxx(pair[0], pair[1], score_only=True) / 300)) - 1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "rng = np.random.default_rng()\n",
    "def randomized_epoch(data, str_data):\n",
    "    x1, x2, x1_str, x2_str = train_test_split(data, str_data, test_size=.5, random_state=0)\n",
    "    \n",
    "    y = np.array([dissimilarity(i) for i in tqdm(zip(x1_str, x2_str), total=x1_str.shape[0])])\n",
    "#     import multiprocessing\n",
    "#     with multiprocessing.Pool() as p:\n",
    "#         y = np.array(list(tqdm(p.imap(dissimilarity, zip(x1_str, x2_str), chunksize=1000), total=x1_str.shape[0])))\n",
    "    \n",
    "    train_data = tf.data.Dataset.from_tensor_slices(({'input_a': x1, 'input_b': x2}, y))\n",
    "    train_data = train_data.batch(100)\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    train_data = train_data.with_options(options)\n",
    "    \n",
    "    fine_siamese.fit(train_data, epochs=1)\n",
    "\n",
    "def validate():\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((\n",
    "        {'input_a': anc_val_x1[:10000], 'input_b': anc_val_x2[:10000]},))\n",
    "    val_data = val_data.batch(100)\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    val_data = val_data.with_options(options)\n",
    "    pred = fine_siamese.predict(val_data)\n",
    "    return np.corrcoef(pred, anc_val_labels[:10000])[0, 1] ** 2, ((pred - anc_val_labels[:10000]) ** 2).mean()\n",
    "\n",
    "def train(epochs):\n",
    "    for i in range(epochs):\n",
    "        print(f'Epoch {i + 1}:')\n",
    "        randomized_epoch(anc_seqs_train, anc_str_seqs_train)\n",
    "        val_r, val_mse = validate()\n",
    "        print(f'val_mse: {val_mse}; val_r2: {val_r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cfffcb3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98cb1af5b764a07a431a6251900ed9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 10s 19ms/step - loss: 0.5055 - mean_absolute_error: 0.2446\n",
      "100/100 [==============================] - 3s 10ms/step\n",
      "val_mse: 0.06252066376104262; val_r2: 0.6816427546050022\n",
      "Epoch 2:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbe6f564f31434799574d4fd197533a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 2s 19ms/step - loss: 0.3729 - mean_absolute_error: 0.2105\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "val_mse: 0.0610471633910062; val_r2: 0.7107947423946692\n",
      "Epoch 3:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890e593423394e4a9cf9838419d50157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 2s 19ms/step - loss: 0.3546 - mean_absolute_error: 0.2052\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "val_mse: 0.052737997422355014; val_r2: 0.7061911952433837\n",
      "Epoch 4:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4b0fbca92c4e388b555b6cc8dbbbb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 2s 19ms/step - loss: 0.3406 - mean_absolute_error: 0.2004\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "val_mse: 0.059985276111745575; val_r2: 0.7226272573397204\n",
      "Epoch 5:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afdebd9e3234068ab46ae24b7a30bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 2s 19ms/step - loss: 0.3338 - mean_absolute_error: 0.1994\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "val_mse: 0.062457244023415855; val_r2: 0.725667252024425\n",
      "Epoch 6:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87681e61046f4beb90c365119d4ac1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 2s 19ms/step - loss: 0.3308 - mean_absolute_error: 0.1969\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "val_mse: 0.050611884612311746; val_r2: 0.7218607896530376\n",
      "Epoch 7:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae5d1954a40401baf3a0391fee301bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 2s 19ms/step - loss: 0.3284 - mean_absolute_error: 0.1961\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "val_mse: 0.055272162841072794; val_r2: 0.725200961371239\n",
      "Epoch 8:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce92f69ca054db58705de04d5b2ef6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 2s 19ms/step - loss: 0.3289 - mean_absolute_error: 0.1978\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "val_mse: 0.05967544963187861; val_r2: 0.7193539895501461\n",
      "Epoch 9:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1fa1832a194be1bd6e4bb08c340152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 2s 19ms/step - loss: 0.3287 - mean_absolute_error: 0.1965\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "val_mse: 0.05736218615570365; val_r2: 0.7209011743586109\n",
      "Epoch 10:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7cbd5195f2433b81bc8017a9e68ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 2s 19ms/step - loss: 0.3324 - mean_absolute_error: 0.1958\n",
      "100/100 [==============================] - 1s 10ms/step\n",
      "val_mse: 0.05674700007609547; val_r2: 0.7255925506486571\n",
      "Epoch 11:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4be330ba68e46f3baed3429e80d5ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1666560/3697265181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1666560/3302834926.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {i + 1}:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mrandomized_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manc_seqs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manc_str_seqs_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mval_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'val_mse: {val_mse}; val_r2: {val_r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1666560/3302834926.py\u001b[0m in \u001b[0;36mrandomized_epoch\u001b[0;34m(data, str_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdissimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx1_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#     import multiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     with multiprocessing.Pool() as p:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1666560/3302834926.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdissimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx1_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#     import multiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     with multiprocessing.Pool() as p:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1666560/3302834926.py\u001b[0m in \u001b[0;36mdissimilarity\u001b[0;34m(pair)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mBio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdissimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpairwise2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocalxx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/Bio/pairwise2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **keywds)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;34m\"\"\"Call the alignment instance already created.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mkeywds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkeywds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkeywds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/Bio/pairwise2.py\u001b[0m in \u001b[0;36m_align\u001b[0;34m(sequenceA, sequenceB, match_fn, gap_A_fn, gap_B_fn, penalize_extend_when_opening, penalize_end_gaps, align_globally, gap_char, force_generic, score_only, one_alignment_only)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mopen_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgap_A_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgap_A_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mopen_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgap_B_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgap_B_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         matrices = _make_score_matrix_fast(\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0msequenceA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0msequenceB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d088c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, multi_head_attention_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn while saving (showing 5 of 46). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, multi_head_attention_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn while saving (showing 5 of 46). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "fine_siamese.save('Models/comparative_encoder/full_model_anc')\n",
    "fine_embeddings.save('Models/comparative_encoder/encoder_anc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70504a9",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0f7cb112",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 16:07:02.106742: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_4047434\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024FlatMapDataset:11455\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536/536 [==============================] - 5s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "anc_seq_reps = fine_embeddings.predict(arlington_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c81442",
   "metadata": {},
   "source": [
    "#### Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4dc6192f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce03b02efb44fdbae5b6c901aaf3452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from scipy.spatial.distance import euclidean\n",
    "codes = BASES + ['N']\n",
    "def to_str(s):\n",
    "    return ''.join(codes[i] for i in s)\n",
    "def evaluate():\n",
    "    a = random.randint(0, anc_seq_reps.shape[0] - 1)\n",
    "    b = random.randint(0, anc_seq_reps.shape[0] - 1)\n",
    "    pred = euclidean(anc_seq_reps[a], anc_seq_reps[b])\n",
    "    first = to_str(arlington_processed[a].argmax(-1))\n",
    "    second = to_str(arlington_processed[b].argmax(-1))\n",
    "    score = dissimilarity((first, second))\n",
    "    return [score, pred]\n",
    "results = np.asarray([evaluate() for i in tqdm(range(10000))])\n",
    "f = plt.figure(figsize=(8, 6))\n",
    "plt.scatter(results[:, 0], results[:, 1], alpha=.2)\n",
    "plt.xlabel('Pairwise Similarity Score (Smith-Waterman)')\n",
    "plt.ylabel('Euclidean Distance Between Encodings')\n",
    "plt.title('Correlation Plot of Model Encodings')\n",
    "plt.savefig('Results/it1/anc/eval/comparative_enc_eval_anc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9976a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.89908656],\n",
       "       [0.89908656, 1.        ]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(results[:, 0], results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "531165aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033395462379869754"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((results[:, 0] - results[:, 1]) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7bd15",
   "metadata": {},
   "source": [
    "#### Full Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06b78200",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = anc_seq_reps.T\n",
    "f = plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, alpha=.1, marker='o')\n",
    "plt.title(\"Encoded Representations of ANC Dataset\")\n",
    "plt.savefig('Results/it1/anc/anc_all.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa34e37",
   "metadata": {},
   "source": [
    "## Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b819aa",
   "metadata": {},
   "source": [
    "### Phylum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "77e4f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phyla, counts = np.unique(arlington_tax[:, 0], return_counts=True)\n",
    "genuses = all_phyla[counts > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f25ef159",
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable = np.isin(arlington_tax[:, 0], genuses)\n",
    "to_plot = np.zeros((np.nonzero(plottable)[0].shape[0], genuses.shape[0]))\n",
    "for i in range(len(genuses)):\n",
    "    to_plot[arlington_tax[plottable][:, 0] == genuses[i], i] = 1\n",
    "plottable_seqs = anc_seq_reps[plottable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fc2d10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in to_plot.T:\n",
    "    pop = plottable_seqs[i.astype(bool)]\n",
    "    samp = rng.integers(0, len(pop), 500)\n",
    "    x, y = zip(*pop[samp])\n",
    "    ax.scatter(x, y, alpha=.3, marker='o')\n",
    "ax.set_title(\"Encoded Representations of Phyla of Bacteria\")\n",
    "leg = plt.legend(genuses,\n",
    "                markerscale=1,\n",
    "                borderpad=1)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "plt.savefig('Results/it1/anc/anc_phylum.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70198de",
   "metadata": {},
   "source": [
    "### Variable Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "01703e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_regions, counts = np.unique(v_region_labels, return_counts=True)\n",
    "genuses = all_regions[counts > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e71c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable = np.isin(v_region_labels, genuses)\n",
    "to_plot = np.zeros((np.nonzero(plottable)[0].shape[0], genuses.shape[0]))\n",
    "for i in range(len(genuses)):\n",
    "    to_plot[v_region_labels[plottable] == genuses[i], i] = 1\n",
    "plottable_seqs = anc_seq_reps[plottable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca2bfdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in to_plot.T:\n",
    "    pop = plottable_seqs[i.astype(bool)]\n",
    "    samp = rng.integers(0, len(pop), 1000)\n",
    "    x, y = zip(*pop[samp])\n",
    "    ax.scatter(x, y, alpha=.2, marker='o')\n",
    "ax.set_title(\"Encoded Representations (Colored by Variable Region)\")\n",
    "leg = plt.legend(genuses,\n",
    "                markerscale=1,\n",
    "                borderpad=1)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "plt.savefig('Results/it1/anc/anc_vregion.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb47bec",
   "metadata": {},
   "source": [
    "### V8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "59e22ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = v_region_labels == 'V8'\n",
    "arlington_tax_sub = arlington_tax[subset]\n",
    "anc_2d_sub = anc_seq_reps[subset]\n",
    "anc_lbl_sub = arlington_labels[subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4b3489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8, 6))\n",
    "plt.scatter(anc_2d_sub[:, 0], anc_2d_sub[:, 1], alpha=.05)\n",
    "plt.title('V8 Sequences from Arlington National Cemetery')\n",
    "plt.savefig('Results/it1/anc/anc_v8.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2c541",
   "metadata": {},
   "source": [
    "#### Phylum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ff044b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phyla, counts = np.unique(arlington_tax_sub[:, 0], return_counts=True)\n",
    "genuses = all_phyla[counts > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0b23fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable = np.isin(arlington_tax_sub[:, 0], genuses)\n",
    "to_plot = np.zeros((np.nonzero(plottable)[0].shape[0], genuses.shape[0]))\n",
    "for i in range(len(genuses)):\n",
    "    to_plot[arlington_tax_sub[plottable][:, 0] == genuses[i], i] = 1\n",
    "plottable_seqs = anc_2d_sub[plottable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e130fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in to_plot.T:\n",
    "    pop = plottable_seqs[i.astype(bool)]\n",
    "    samp = rng.integers(0, len(pop), 1000)\n",
    "    x, y = zip(*pop[samp])\n",
    "    ax.scatter(x, y, alpha=.3, marker='o')\n",
    "ax.set_title(\"Encoded Representations of Phyla of Bacteria (V8 Only)\")\n",
    "leg = plt.legend(genuses,\n",
    "                markerscale=1,\n",
    "                borderpad=1)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "plt.savefig('Results/it1/anc/anc_v8_phylum.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6732354",
   "metadata": {},
   "source": [
    "#### Pigment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2bd75671",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phyla, counts = np.unique(anc_lbl_sub, return_counts=True)\n",
    "genuses = all_phyla[counts > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "74718315",
   "metadata": {},
   "outputs": [],
   "source": [
    "plottable = np.isin(anc_lbl_sub, genuses)\n",
    "to_plot = np.zeros((np.nonzero(plottable)[0].shape[0], genuses.shape[0]))\n",
    "for i in range(len(genuses)):\n",
    "    to_plot[anc_lbl_sub[plottable] == genuses[i], i] = 1\n",
    "plottable_seqs = anc_2d_sub[plottable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7c7dff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "black, grey, unpigmented = anc_lbl_sub == 'black', anc_lbl_sub == 'grey', anc_lbl_sub == 'unpigmented'\n",
    "plt.scatter(anc_2d_sub[black][:, 0], anc_2d_sub[black][:, 1], alpha=1)\n",
    "plt.scatter(anc_2d_sub[grey][:, 0], anc_2d_sub[grey][:, 1], alpha=.2)\n",
    "plt.scatter(anc_2d_sub[unpigmented][:, 0], anc_2d_sub[unpigmented][:, 1], alpha=.1)\n",
    "ax.set_title(\"Encoded Representations of V8 Bacteria, by Pigmentation\")\n",
    "leg = plt.legend(['Black', 'Grey', 'Unpigmented'],\n",
    "                markerscale=1,\n",
    "                borderpad=1)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "plt.savefig('Results/it1/anc/anc_v8_pigment.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26b155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
